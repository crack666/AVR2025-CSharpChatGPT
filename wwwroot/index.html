<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Voice Chat Assistant</title>
  <style>
    body { font-family: Arial, sans-serif; margin: 2em; }
    label { display: block; margin-top: 1em; }
    button { margin-top: 1em; padding: 0.5em 1em; }
    #chatLog { border: 1px solid #ccc; padding: 1em; max-height: 400px; overflow-y: auto; margin-top: 1em; }
  </style>
</head>
<body>
  <h1>Voice Chat Assistant</h1>
  <label>Model:
    <select id="model"></select>
  </label>
  <p>Aktuelles Modell: <span id="currentModel"></span></p>
  <label>Speech Language:
    <select id="language"></select>
  </label>
  <label>Voice:
    <select id="voice"></select>
  </label>
  <label>Transcription Mode:
    <select id="asrMode">
      <option value="whisper">Whisper (Server)</option>
      <option value="browser">Browser ASR</option>
    </select>
  </label>
  <!-- Silence threshold duration in seconds -->
  <label>Stille-Schwelle (s):
    <input type="number" id="silenceSec" min="0.5" max="10" step="0.1" value="2" />
  </label>
  <!-- Continuous listening; no manual record button -->
  <p id="status"></p>
  <div id="chatLog"></div>
  <script>
    const status = document.getElementById('status');
    const chatLog = document.getElementById('chatLog');
    const modelSel = document.getElementById('model');
    const langSel = document.getElementById('language');
    const voiceSel = document.getElementById('voice');
    const silenceSecInput = document.getElementById('silenceSec');
    const asrMode = document.getElementById('asrMode');
    // Dynamically load available chat models from server
    async function loadModels() {
      modelSel.innerHTML = '';
      try {
        const res = await fetch('/api/models');
        if (!res.ok) throw new Error(`HTTP ${res.status}`);
        const models = await res.json();
        models.forEach(m => {
          const opt = document.createElement('option'); opt.value = m; opt.textContent = m;
          modelSel.appendChild(opt);
        });
      } catch (err) {
        console.error('Fehler beim Laden der Modelle:', err);
        ['gpt-3.5-turbo', 'gpt-4'].forEach(m => {
          const opt = document.createElement('option'); opt.value = m; opt.textContent = m;
          modelSel.appendChild(opt);
        });
      }
    }
    const loadPromise = loadModels();
    const currentModelEl = document.getElementById('currentModel');
    modelSel.addEventListener('change', () => { currentModelEl.textContent = modelSel.value; });
    loadPromise.then(() => { currentModelEl.textContent = modelSel.value; });

    // Populate browser voices and OpenAI TTS voices
    const openaiVoices = ['nova', 'shimmer', 'echo', 'onyx', 'fable', 'alloy', 'ash', 'sage', 'coral'];
    function populateVoices() {
      // Speech language dropdown
      const voices = speechSynthesis.getVoices();
      langSel.innerHTML = '';
      voiceSel.innerHTML = '';
      const languages = [...new Set(voices.map(v => v.lang))];
      languages.forEach(lang => {
        const opt = document.createElement('option'); opt.value = lang; opt.textContent = lang;
        langSel.appendChild(opt);
      });
      // OpenAI TTS voice options
      openaiVoices.forEach(v => {
        const opt = document.createElement('option');
        opt.value = v;
        opt.textContent = `OpenAI ${v.charAt(0).toUpperCase() + v.slice(1)}`;
        voiceSel.appendChild(opt);
      });
      // Browser-native voices
      voices.forEach(voice => {
        const opt = document.createElement('option');
        opt.value = voice.name;
        opt.textContent = `${voice.name} (${voice.lang})`;
        voiceSel.appendChild(opt);
      });
    }
    speechSynthesis.onvoiceschanged = populateVoices;
    populateVoices();

    // Helper to append chat messages
    function appendChat(data) {
      const userMessage = document.createElement('p');
      userMessage.innerHTML = `<strong>Du:</strong> ${data.prompt}`;
      chatLog.appendChild(userMessage);
      const assistantMessage = document.createElement('p');
      assistantMessage.innerHTML = `<strong>Assistant (${data.model}):</strong> ${data.response}`;
      chatLog.appendChild(assistantMessage);
      chatLog.scrollTop = chatLog.scrollHeight;
    }

    // Helper to speak response with TTS or SpeechSynthesis
    async function speakResponse(text) {
      if (openaiVoices.includes(voiceSel.value)) {
        status.textContent = 'Synthesizing...';
        const resp2 = await fetch('/api/speech', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({ Input: text, Voice: voiceSel.value })
        });
        if (!resp2.ok) {
          const errText = await resp2.text();
          status.textContent = `Error synthesizing speech: ${resp2.status} ${errText}`;
          return;
        }
        const audioBlob = await resp2.blob();
        const audioUrl = URL.createObjectURL(audioBlob);
        const audio = new Audio(audioUrl);
        await new Promise(resolve => {
          audio.onended = () => { URL.revokeObjectURL(audioUrl); resolve(); };
          audio.play();
        });
      } else {
        status.textContent = 'Speaking...';
        await new Promise(resolve => {
          const utter = new SpeechSynthesisUtterance(text);
          utter.lang = langSel.value;
          const selectedVoice = speechSynthesis.getVoices().find(v => v.name === voiceSel.value);
          if (selectedVoice) utter.voice = selectedVoice;
          utter.onend = () => resolve();
          speechSynthesis.speak(utter);
        });
      }
    }

    if (asrMode.value === 'browser' && (window.SpeechRecognition || window.webkitSpeechRecognition)) {
      // Streaming ASR via Web Speech API
      (function() {
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        const recognition = new SpeechRecognition();
        recognition.continuous = true;
        recognition.interimResults = true;
        recognition.lang = langSel.value;
        recognition.onstart = () => { status.textContent = 'Listening (ASR)...'; };
        recognition.onerror = (e) => { console.error('Speech recognition error', e); status.textContent = 'Error in recognition'; };
        recognition.onresult = async (event) => {
          let interimTranscript = '';
          let finalTranscript = '';
          for (let i = event.resultIndex; i < event.results.length; i++) {
            const result = event.results[i];
            const transcript = result[0].transcript;
            if (result.isFinal) finalTranscript += transcript;
            else interimTranscript += transcript;
          }
          if (interimTranscript) {
            status.textContent = interimTranscript;
          }
          if (finalTranscript) {
            status.textContent = 'Processing...';
            await sendChat(finalTranscript.trim());
            status.textContent = 'Listening (ASR)...';
          }
        };
        recognition.onend = () => recognition.start();
        recognition.start();

        async function sendChat(prompt) {
          try {
            status.textContent = 'Connecting...';
            const resp = await fetch('/api/chatStream', {
              method: 'POST',
              headers: { 'Content-Type': 'application/json' },
              body: JSON.stringify({ model: modelSel.value, prompt })
            });
            if (!resp.ok) { status.textContent = `Error: ${resp.status}`; return; }
            const reader = resp.body.getReader();
            const decoder = new TextDecoder();
            let buffer = '';
            let content = '';
            const assistantElem = document.createElement('p');
            const userMessage = document.createElement('p');
            userMessage.innerHTML = `<strong>Du:</strong> ${prompt}`;
            chatLog.appendChild(userMessage);
            chatLog.appendChild(assistantElem);
            while (true) {
              const { value, done } = await reader.read();
              if (done) break;
              buffer += decoder.decode(value, { stream: true });
              const parts = buffer.split('\n\n');
              buffer = parts.pop();
              for (const part of parts) {
                const lines = part.split('\n');
                let eventType = '';
                let dataLine = '';
                for (const line of lines) {
                  if (line.startsWith('event: ')) eventType = line.slice(7);
                  else if (line.startsWith('data: ')) dataLine = line.slice(6);
                }
                if (eventType === 'prompt') continue;
                if (dataLine) {
                  const obj = JSON.parse(dataLine);
                  content += obj.token;
                  assistantElem.innerHTML = `<strong>Assistant (${modelSel.value}):</strong> ${content}`;
                  chatLog.scrollTop = chatLog.scrollHeight;
                }
                if (eventType === 'done') {
                  await speakResponse(content);
                }
              }
            }
          } catch (err) {
            console.error(err);
            status.textContent = 'Error in ASR chat';
          }
        }
      })();
    } else {
      if (asrMode.value === 'browser') {
        status.textContent = 'Browser ASR not supported, falling back to Whisper (server)';
      }
      // Continuous recording with silence detection
    (async () => {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      const audioContext = new AudioContext();
      const source = audioContext.createMediaStreamSource(stream);
      const analyser = audioContext.createAnalyser();
      // Reduce FFT size for lower-latency voice activity detection
      analyser.fftSize = 1024;
      source.connect(analyser);
      const dataArray = new Uint8Array(analyser.fftSize);
      const recorder = new MediaRecorder(stream);
      let chunks = [];
      recorder.ondataavailable = e => chunks.push(e.data);
      recorder.onstop = async () => {
        status.textContent = 'Uploading...';
        const blob = new Blob(chunks, { type: 'audio/webm' });
        const fd = new FormData();
        fd.append('file', blob, 'audio.webm');
        fd.append('model', modelSel.value);
        fd.append('language', langSel.value);
        try {
          status.textContent = 'Connecting...';
          const resp = await fetch('/api/processAudioStream', { method: 'POST', body: fd });
          if (!resp.ok) { status.textContent = `Error: ${resp.status}`; return; }
          const reader = resp.body.getReader();
          const decoder = new TextDecoder();
          let buffer = '';
          let content = '';
          const assistantElem = document.createElement('p');
          chatLog.appendChild(assistantElem);
          // Parse SSE stream
          while (true) {
            const { value, done: doneReading } = await reader.read();
            if (doneReading) break;
            buffer += decoder.decode(value, { stream: true });
            const parts = buffer.split('\n\n');
            buffer = parts.pop();
            for (const part of parts) {
              const lines = part.split('\n');
              let eventType = '';
              let dataLine = '';
              for (const line of lines) {
                if (line.startsWith('event: ')) eventType = line.slice(7);
                else if (line.startsWith('data: ')) dataLine = line.slice(6);
              }
              if (eventType === 'prompt') {
                const obj = JSON.parse(dataLine);
                const userMessage = document.createElement('p');
                userMessage.innerHTML = `<strong>Du:</strong> ${obj.prompt}`;
                chatLog.appendChild(userMessage);
              } else if (!eventType || eventType === 'message') {
                const obj = JSON.parse(dataLine);
                content += obj.token;
                assistantElem.innerHTML = `<strong>Assistant (${modelSel.value}):</strong> ${content}`;
              } else if (eventType === 'done') {
                await speakResponse(content);
                status.textContent = 'Listening...';
                return;
              }
            }
          }
        } catch (err) {
          console.error(err);
          status.textContent = 'Error in processing';
        }
        status.textContent = 'Listening...';
      };
      let speakingSegment = false;
      let silenceStart = null;
      const silenceThreshold = 0.02;
      status.textContent = 'Listening...';
      // Poll more frequently for quicker reaction (ms)
      setInterval(() => {
        analyser.getByteTimeDomainData(dataArray);
        let sumSquares = 0;
        for (let i = 0; i < dataArray.length; i++) {
          const v = (dataArray[i] - 128) / 128;
          sumSquares += v * v;
        }
        const rms = Math.sqrt(sumSquares / dataArray.length);
        if (rms > silenceThreshold) {
          if (!speakingSegment) {
            speakingSegment = true;
            chunks = [];
            recorder.start();
            status.textContent = 'Recording...';
          }
          silenceStart = null;
        } else {
          if (speakingSegment) {
            if (!silenceStart) silenceStart = Date.now();
            else if (Date.now() - silenceStart > parseFloat(silenceSecInput.value) * 1000) {
              speakingSegment = false;
              recorder.stop();
            }
          }
        }
      }, 50);
    })();
    }
  </script>
</body>
</html>