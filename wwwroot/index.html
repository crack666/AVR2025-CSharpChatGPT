<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Voice Chat Assistant</title>
  <style>
    body { font-family: Arial, sans-serif; margin: 2em; }
    label { display: block; margin-top: 1em; }
    button { margin-top: 1em; padding: 0.5em 1em; }
    #chatLog { border: 1px solid #ccc; padding: 1em; max-height: 400px; overflow-y: auto; margin-top: 1em; }
  </style>
</head>
<body>
  <h1>Voice Chat Assistant</h1>
  <label>Model:
    <select id="model"></select>
  </label>
  <p>Aktuelles Modell: <span id="currentModel"></span></p>
  <label>Speech Language:
    <select id="language"></select>
  </label>
  <label>Voice:
    <select id="voice"></select>
  </label>
  <label>Transcription Mode:
    <select id="asrMode">
      <option value="whisper">Whisper (Server)</option>
      <option value="browser">Browser ASR</option>
    </select>
  </label>
  <!-- Silence threshold duration in seconds -->
  <label>Stille-Schwelle (s):
    <input type="number" id="silenceSec" min="0.5" max="10" step="0.1" value="2" />
  </label>
  <!-- Continuous listening; no manual record button -->
  <p id="status"></p>
  <button id="stopBtn">Stopp</button>
  <div id="chatLog"></div>
  <script>
    const status = document.getElementById('status');
    const stopBtn = document.getElementById('stopBtn');
    stopBtn.addEventListener('click', () => {
      if (currentAudio) {
        currentAudio.pause();
        URL.revokeObjectURL(currentAudio.src);
        currentAudio = null;
      }
      if (currentUtterance) {
        speechSynthesis.cancel();
        currentUtterance = null;
      }
      status.textContent = 'Stopped';
    });
    let currentAudio = null;
    let currentUtterance = null;
    const chatLog = document.getElementById('chatLog');
    const modelSel = document.getElementById('model');
    const langSel = document.getElementById('language');
    const voiceSel = document.getElementById('voice');
    const silenceSecInput = document.getElementById('silenceSec');
    const asrMode = document.getElementById('asrMode');
    // Dynamically load available chat models from server
    async function loadModels() {
      modelSel.innerHTML = '';
      try {
        const res = await fetch('/api/models');
        if (!res.ok) throw new Error(`HTTP ${res.status}`);
        const models = await res.json();
        models.forEach(m => {
          const opt = document.createElement('option'); opt.value = m; opt.textContent = m;
          modelSel.appendChild(opt);
        });
      } catch (err) {
        console.error('Fehler beim Laden der Modelle:', err);
        ['gpt-3.5-turbo', 'gpt-4'].forEach(m => {
          const opt = document.createElement('option'); opt.value = m; opt.textContent = m;
          modelSel.appendChild(opt);
        });
      }
    }
    const loadPromise = loadModels();
    const currentModelEl = document.getElementById('currentModel');
    modelSel.addEventListener('change', () => { currentModelEl.textContent = modelSel.value; });
    loadPromise.then(() => { currentModelEl.textContent = modelSel.value; });

    // Populate browser voices and OpenAI TTS voices
    const openaiVoices = ['nova', 'shimmer', 'echo', 'onyx', 'fable', 'alloy', 'ash', 'sage', 'coral'];
    function populateVoices() {
      // Speech language dropdown
      const voices = speechSynthesis.getVoices();
      langSel.innerHTML = '';
      voiceSel.innerHTML = '';
      const languages = [...new Set(voices.map(v => v.lang))];
      languages.forEach(lang => {
        const opt = document.createElement('option'); opt.value = lang; opt.textContent = lang;
        langSel.appendChild(opt);
      });
      // OpenAI TTS voice options
      openaiVoices.forEach(v => {
        const opt = document.createElement('option');
        opt.value = v;
        opt.textContent = `OpenAI ${v.charAt(0).toUpperCase() + v.slice(1)}`;
        voiceSel.appendChild(opt);
      });
      // Browser-native voices
      voices.forEach(voice => {
        const opt = document.createElement('option');
        opt.value = voice.name;
        opt.textContent = `${voice.name} (${voice.lang})`;
        voiceSel.appendChild(opt);
      });
    }
    speechSynthesis.onvoiceschanged = populateVoices;
    populateVoices();

    // Helper to append chat messages
    function appendChat(data) {
      const userMessage = document.createElement('p');
      userMessage.innerHTML = `<strong>Du:</strong> ${data.prompt}`;
      chatLog.appendChild(userMessage);
      const assistantMessage = document.createElement('p');
      assistantMessage.innerHTML = `<strong>Assistant (${data.model}):</strong> ${data.response}`;
      chatLog.appendChild(assistantMessage);
      chatLog.scrollTop = chatLog.scrollHeight;
    }

    // Helper to speak response with TTS or SpeechSynthesis
    async function speakResponse(text) {
      // Validate text input
      if (!text || text.trim().length === 0) {
        console.warn("Attempted to synthesize empty text");
        status.textContent = 'No text to synthesize';
        return;
      }

      // Abort any previous playback
      if (currentAudio) {
        currentAudio.pause();
        URL.revokeObjectURL(currentAudio.src);
        currentAudio = null;
      }
      if (currentUtterance) {
        speechSynthesis.cancel();
        currentUtterance = null;
      }

      // Ensure text is properly trimmed
      text = text.trim();
      
      if (openaiVoices.includes(voiceSel.value)) {
        status.textContent = 'Synthesizing...';
        try {
          const resp2 = await fetch('/api/speech', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({ Input: text, Voice: voiceSel.value })
          });
          if (!resp2.ok) {
            const errText = await resp2.text();
            status.textContent = `Error synthesizing speech: ${resp2.status} ${errText}`;
            return;
          }
          const audioBlob = await resp2.blob();
          const audioUrl = URL.createObjectURL(audioBlob);
          const audio = new Audio(audioUrl);
          currentAudio = audio;
          await new Promise(resolve => {
            audio.onended = () => { URL.revokeObjectURL(audioUrl); currentAudio = null; resolve(); };
            audio.play();
          });
        } catch (error) {
          console.error("TTS error:", error);
          status.textContent = `Speech synthesis error: ${error.message}`;
        }
      } else {
        // Browser Speech Synthesis
        status.textContent = 'Speaking...';
        try {
          await new Promise(resolve => {
            const utter = new SpeechSynthesisUtterance(text);
            currentUtterance = utter;
            utter.lang = langSel.value;
            const selectedVoice = speechSynthesis.getVoices().find(v => v.name === voiceSel.value);
            if (selectedVoice) utter.voice = selectedVoice;
            utter.onend = () => { currentUtterance = null; resolve(); };
            utter.onerror = (e) => { 
              console.error("SpeechSynthesis error:", e);
              currentUtterance = null;
              resolve();
            };
            speechSynthesis.speak(utter);
          });
        } catch (error) {
          console.error("Browser speech synthesis error:", error);
          status.textContent = `Browser speech error: ${error.message}`;
        }
      }
    }

    if (asrMode.value === 'browser' && (window.SpeechRecognition || window.webkitSpeechRecognition)) {
      // Streaming ASR via Web Speech API
      (function() {
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        const recognition = new SpeechRecognition();
        recognition.continuous = true;
        recognition.interimResults = true;
        recognition.lang = langSel.value;
        recognition.onstart = () => { status.textContent = 'Listening (ASR)...'; };
        recognition.onerror = (e) => { console.error('Speech recognition error', e); status.textContent = 'Error in recognition'; };
        recognition.onresult = async (event) => {
          let interimTranscript = '';
          let finalTranscript = '';
          for (let i = event.resultIndex; i < event.results.length; i++) {
            const result = event.results[i];
            const transcript = result[0].transcript;
            if (result.isFinal) finalTranscript += transcript;
            else interimTranscript += transcript;
          }
          if (interimTranscript) {
            status.textContent = interimTranscript;
          }
          if (finalTranscript) {
            status.textContent = 'Processing...';
            await sendChat(finalTranscript.trim());
            status.textContent = 'Listening (ASR)...';
          }
        };
        recognition.onend = () => recognition.start();
        recognition.start();

        async function sendChat(prompt) {
          try {
            status.textContent = 'Connecting...';
            const resp = await fetch('/api/chatStream', {
              method: 'POST',
              headers: { 'Content-Type': 'application/json' },
              body: JSON.stringify({ model: modelSel.value, prompt })
            });
            if (!resp.ok) { status.textContent = `Error: ${resp.status}`; return; }
            const reader = resp.body.getReader();
            const decoder = new TextDecoder();
            let buffer = '';
            let content = '';
            const assistantElem = document.createElement('p');
            const userMessage = document.createElement('p');
            userMessage.innerHTML = `<strong>Du:</strong> ${prompt}`;
            chatLog.appendChild(userMessage);
            chatLog.appendChild(assistantElem);
            while (true) {
              const { value, done } = await reader.read();
              if (done) break;
              buffer += decoder.decode(value, { stream: true });
              const parts = buffer.split('\n\n');
              buffer = parts.pop();
              for (const part of parts) {
                const lines = part.split('\n');
                let eventType = '';
                let dataLine = '';
                for (const line of lines) {
                  if (line.startsWith('event: ')) eventType = line.slice(7);
                  else if (line.startsWith('data: ')) dataLine = line.slice(6);
                }
                if (eventType === 'prompt') continue;
                if (dataLine) {
                  const obj = JSON.parse(dataLine);
                  if (obj.token !== undefined) {
                    // Token-by-token streaming (optimized)
                    content += obj.token;
                    // Update UI more efficiently for token streaming
                    requestAnimationFrame(() => {
                      assistantElem.innerHTML = `<strong>Assistant (${modelSel.value}):</strong> ${content}`;
                      chatLog.scrollTop = chatLog.scrollHeight;
                    });
                  } else if (obj.message !== undefined) {
                    content += obj.message;
                    assistantElem.innerHTML = `<strong>Assistant (${modelSel.value}):</strong> ${content}`;
                    chatLog.scrollTop = chatLog.scrollHeight;
                  } else if (obj.response !== undefined) {
                    content += obj.response;
                    assistantElem.innerHTML = `<strong>Assistant (${modelSel.value}):</strong> ${content}`;
                    chatLog.scrollTop = chatLog.scrollHeight;
                  }
                }
                if (eventType === 'done') {
                  await speakResponse(content);
                }
              }
            }
          } catch (err) {
            console.error(err);
            status.textContent = 'Error in ASR chat';
          }
        }
      })();
    } else {
      if (asrMode.value === 'browser') {
        status.textContent = 'Browser ASR not supported, falling back to Whisper (server)';
      }
      // Continuous recording with silence detection
    (async () => {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      const audioContext = new AudioContext();
      const source = audioContext.createMediaStreamSource(stream);
      const analyser = audioContext.createAnalyser();
      // Reduce FFT size for lower-latency voice activity detection
      analyser.fftSize = 1024;
      source.connect(analyser);
      const dataArray = new Uint8Array(analyser.fftSize);
      const recorder = new MediaRecorder(stream);
      let chunks = [];
      recorder.ondataavailable = e => chunks.push(e.data);
      recorder.onstop = async () => {
        status.textContent = 'Uploading...';
        const blob = new Blob(chunks, { type: 'audio/webm' });
        const fd = new FormData();
        fd.append('file', blob, 'audio.webm');
        fd.append('model', modelSel.value);
        fd.append('language', langSel.value);
        try {
          console.log("Uploading audio for processing...");
          status.textContent = 'Connecting...';
          const resp = await fetch('/api/processAudioStream', { method: 'POST', body: fd });
          
          if (!resp.ok) { 
            const errorText = await resp.text();
            console.error(`Server error: ${resp.status}, ${errorText}`);
            status.textContent = `Error: ${resp.status}`; 
            return; 
          }
          
          console.log("Connection established, processing response stream...");
          const reader = resp.body.getReader();
          const decoder = new TextDecoder();
          let buffer = '';
          let content = '';
          const assistantElem = document.createElement('p');
          chatLog.appendChild(assistantElem);
          
          // Flag to track if we've received any tokens
          let receivedAnyTokens = false;
          
          // Parse SSE stream
          while (true) {
            const { value, done: doneReading } = await reader.read();
            if (doneReading) {
              console.log("Stream reading done");
              break;
            }
            
            const chunk = decoder.decode(value, { stream: true });
            console.log(`Received chunk: ${chunk.length} bytes`);
            buffer += chunk;
            
            const parts = buffer.split('\n\n');
            buffer = parts.pop();
            
            console.log(`Processing ${parts.length} SSE parts`);
            
            for (const part of parts) {
              const lines = part.split('\n');
              let eventType = '';
              let dataLine = '';
              
              for (const line of lines) {
                if (line.startsWith('event: ')) {
                  eventType = line.slice(7);
                  console.log(`Event type: ${eventType}`);
                }
                else if (line.startsWith('data: ')) {
                  dataLine = line.slice(6);
                }
              }
              
              if (eventType === 'prompt') {
                try {
                  const obj = JSON.parse(dataLine);
                  console.log(`Received prompt: "${obj.prompt}"`);
                  const userMessage = document.createElement('p');
                  userMessage.innerHTML = `<strong>Du:</strong> ${obj.prompt}`;
                  chatLog.appendChild(userMessage);
                } catch (e) {
                  console.error(`Error parsing prompt data: ${e.message}`);
                }
              } else if (eventType === 'token') {
                try {
                  const obj = JSON.parse(dataLine);
                  if (obj.token !== undefined) {
                    receivedAnyTokens = true;
                    // Token-by-token streaming (optimized)
                    content += obj.token;
                    // Update UI more efficiently for token streaming
                    requestAnimationFrame(() => {
                      assistantElem.innerHTML = `<strong>Assistant (${modelSel.value}):</strong> ${content}`;
                      chatLog.scrollTop = chatLog.scrollHeight;
                    });
                  }
                } catch (e) {
                  console.error(`Error parsing token data: ${e.message}`);
                }
              } else if (!eventType || eventType === 'message') {
                try {
                  const obj = JSON.parse(dataLine);
                  console.log("Received message data:", obj);
                  
                  if (obj.token !== undefined) {
                    receivedAnyTokens = true;
                    // Token-by-token streaming (optimized)
                    content += obj.token;
                    // Update UI more efficiently for token streaming
                    requestAnimationFrame(() => {
                      assistantElem.innerHTML = `<strong>Assistant (${modelSel.value}):</strong> ${content}`;
                      chatLog.scrollTop = chatLog.scrollHeight;
                    });
                  } else if (obj.message !== undefined) {
                    receivedAnyTokens = true;
                    content += obj.message;
                    assistantElem.innerHTML = `<strong>Assistant (${modelSel.value}):</strong> ${content}`;
                    chatLog.scrollTop = chatLog.scrollHeight;
                  } else if (obj.response !== undefined) {
                    receivedAnyTokens = true;
                    content += obj.response;
                    assistantElem.innerHTML = `<strong>Assistant (${modelSel.value}):</strong> ${content}`;
                    chatLog.scrollTop = chatLog.scrollHeight;
                  }
                } catch (e) {
                  console.error(`Error parsing message data: ${e.message}, raw data: ${dataLine}`);
                }
              } else if (eventType === 'done') {
                console.log(`Stream complete, received ${content.length} characters`);
                
                if (content && content.length > 0) {
                  await speakResponse(content);
                } else {
                  console.error("Empty response content, nothing to speak");
                  // Show a helpful message
                  assistantElem.innerHTML = `<strong>Assistant:</strong> <em>No response generated. Please try again.</em>`;
                }
                
                status.textContent = 'Listening...';
                status.style.fontWeight = 'normal';
                return;
              } else if (eventType === 'error') {
                try {
                  const obj = JSON.parse(dataLine);
                  console.error("Server error:", obj.error);
                  status.textContent = `Error: ${obj.error}`;
                } catch (e) {
                  console.error(`Error parsing error event: ${e.message}`);
                }
                return;
              }
            }
          }
          
          // If we reached end of stream without 'done' event
          console.log("Stream ended without done event");
          if (receivedAnyTokens) {
            await speakResponse(content);
          } else {
            console.error("No tokens received in the response");
            assistantElem.innerHTML = `<strong>Assistant:</strong> <em>No response received. Please try again.</em>`;
          }
          
          status.textContent = 'Listening...';
          status.style.fontWeight = 'normal';
        } catch (err) {
          console.error(err);
          status.textContent = 'Error in processing';
        }
        status.textContent = 'Listening...';
      };
      let speakingSegment = false;
      let silenceStart = null;
      const silenceThreshold = 0.02;
      status.textContent = 'Listening...';
      // Poll more frequently for quicker reaction (ms)
      setInterval(() => {
        analyser.getByteTimeDomainData(dataArray);
        let sumSquares = 0;
        for (let i = 0; i < dataArray.length; i++) {
          const v = (dataArray[i] - 128) / 128;
          sumSquares += v * v;
        }
        const rms = Math.sqrt(sumSquares / dataArray.length);
        
        // Debug output for audio levels
        if (window._debugAudioLevels) {
          console.log(`Audio level: ${rms.toFixed(4)}, Threshold: ${silenceThreshold}`);
        }
        
        if (rms > silenceThreshold) {
          if (!speakingSegment) {
            speakingSegment = true;
            chunks = [];
            recorder.start();
            status.textContent = 'Recording...';
            console.log("Recording started - audio level above threshold");
          }
          silenceStart = null;
        } else {
          if (speakingSegment) {
            if (!silenceStart) {
              silenceStart = Date.now();
              console.log("Silence detected, starting silence timer");
            }
            else if (Date.now() - silenceStart > parseFloat(silenceSecInput.value) * 1000) {
              speakingSegment = false;
              console.log(`Stopping recording after ${parseFloat(silenceSecInput.value)} seconds of silence`);
              
              // Force a more obvious visual feedback
              status.textContent = 'Processing...';
              status.style.fontWeight = 'bold';
              
              recorder.stop();
            }
          }
        }
      }, 50);
      
      // Add debug toggle button
      const debugButton = document.createElement('button');
      debugButton.textContent = 'Toggle Audio Debug';
      debugButton.style.marginTop = '10px';
      debugButton.onclick = () => {
        window._debugAudioLevels = !window._debugAudioLevels;
        debugButton.textContent = window._debugAudioLevels ? 'Disable Audio Debug' : 'Enable Audio Debug';
      };
      document.body.appendChild(debugButton);
    })();
    }
  </script>
</body>
</html>